{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.misc\n",
    "from logging import getLogger\n",
    "import datetime\n",
    "import dateutil.tz\n",
    "from datetime import date\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "import pprint\n",
    "import tarfile\n",
    "\n",
    "import scipy.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST Data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n",
    "valX, valY = mnist.validation.images, mnist.validation.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, sess, num_visible, num_classes, hparams, model_name, data_type, logs_dir):        \n",
    "        self.sess = sess\n",
    "        \n",
    "        self.num_visible = num_visible\n",
    "        self.num_classes = num_classes        \n",
    "        self.num_hidden = hparams['num_hidden']\n",
    "        \n",
    "        self.hparams = {}\n",
    "        \n",
    "        self.hparams['batch_size'] = hparams['batch_size']\n",
    "        self.hparams['num_epochs'] = hparams['num_epochs']\n",
    "        self.hparams['learning_rate'] = hparams['learning_rate']\n",
    "        self.seed = hparams['seed']\n",
    "        \n",
    "        self.gen = hparams['gen']\n",
    "        self.discr = hparams['discr']\n",
    "        \n",
    "        self.batch_normalization = 1.0 / batch_size\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.data_type = data_type\n",
    "        \n",
    "        self.stats = {}\n",
    "        self.stats['train_batch_acc'] = {}\n",
    "        self.stats['train_mean_acc'] = np.zeros(self.hparams['num_epochs'])\n",
    "        self.stats['val_batch_acc'] = {}\n",
    "        self.stats['val_mean_acc'] = np.zeros(self.hparams['num_epochs'])\n",
    "        \n",
    "        self.logs_dir = logs_dir\n",
    "        \n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        self.learning_rate = None\n",
    "        \n",
    "        self.W = None\n",
    "        self.U = None\n",
    "        self.b = None\n",
    "        self.c = None\n",
    "        self.d = None\n",
    "        \n",
    "        self.d_U = None\n",
    "        self.d_W = None\n",
    "        self.d_b = None\n",
    "        self.d_c = None\n",
    "        self.d_d = None\n",
    "        \n",
    "        self.updates = []\n",
    "    \n",
    "    def _build_model(self):\n",
    "        with tf.variable_scope(self.model_name):            \n",
    "            self.X, self.Y, self.learning_rate = self._create_placeholders()                        \n",
    "            \n",
    "            m = np.max([self.num_hidden, self.num_classes])\n",
    "            m_sqrt = np.sqrt(m)\n",
    "            \n",
    "            self.U, self.W, self.b, self.c, self.d = self._initialize_variables(m_sqrt)                        \n",
    "            \n",
    "            self.model = self._construct_model()\n",
    "            \n",
    "            self.d_U, self.d_W, self.d_b, self.d_c, self.d_d = self._define_gradients()\n",
    "            \n",
    "            self.updates = [self.U.assign_add(self.hparams['learning_rate'] * self.d_U),\n",
    "                            self.W.assign_add(self.hparams['learning_rate'] * self.d_W),\n",
    "                            self.b.assign_add(self.hparams['learning_rate'] * self.d_b),\n",
    "                            self.c.assign_add(self.hparams['learning_rate'] * self.d_c),\n",
    "                            self.d.assign_add(self.hparams['learning_rate'] * self.d_d)]\n",
    "            \n",
    "            self.predicted_y = tf.argmax(self.model['p_y_all_given_x'], 1)\n",
    "            self.ground_truth = tf.argmax(self.Y, 1)\n",
    "            \n",
    "            \n",
    "            with tf.name_scope('accuracy'):\n",
    "                with tf.name_scope('correct_prediction'):\n",
    "                    self.correct_prediction = tf.equal(self.predicted_y, self.ground_truth)     \n",
    "                with tf.name_scope('accuracy'):\n",
    "                    self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32),keep_dims=True)                     \n",
    "            tf.summary.scalar('accuracy', self.accuracy[0])\n",
    "            \n",
    "            # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "            self.merged = tf.merge_all_summaries()\n",
    "            self.train_writer = tf.train.SummaryWriter(self.logs_dir + '/train', sess.graph)\n",
    "            self.test_writer = tf.train.SummaryWriter(self.logs_dir + '/test')\n",
    "            \n",
    "            tf.initialize_all_variables().run()\n",
    "            \n",
    "                    \n",
    "                \n",
    "            \n",
    "    def _get_timestamp(self):\n",
    "        now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
    "        return now.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "    \n",
    "    def gen_image(img, width, height, outfile, img_type='grey'):\n",
    "        assert len(img) == width * height or len(img) == width * height * 3\n",
    "\n",
    "        if img_type == 'grey':\n",
    "            misc.imsave(outfile, img.reshape(width, height))\n",
    "\n",
    "        elif img_type == 'color':\n",
    "            misc.imsave(outfile, img.reshape(3, width, height))\n",
    "    \n",
    "    def get_samples(self, height, width):\n",
    "        weights = self.W.eval()\n",
    "        rows, _ = weights.shape\n",
    "        samples = np.asarray([np.reshape(weights[i,:], [height, width]) for i in range(rows)])\n",
    "        \n",
    "        return samples        \n",
    "            \n",
    "    def save_images(self, images, height, width, n_row, n_col, \n",
    "                    cmin=0.0, cmax=1.0, directory=\"./\", prefix=\"sample\"):\n",
    "        images = images.reshape((n_row, n_col, height, width))\n",
    "        images = images.transpose(1, 2, 0, 3)\n",
    "        images = images.reshape((height * n_row, width * n_col))\n",
    "        filename = '%s_%s.jpg' % (prefix, self._get_timestamp())\n",
    "        scipy.misc.toimage(images, cmin=cmin, cmax=cmax).save(os.path.join(directory, filename))\n",
    "    \n",
    "    def save_samples(self, samples_dir, epoch, height, width):\n",
    "        samples = self.get_samples(width, height)\n",
    "\n",
    "        self.save_images(samples, height, width, 50, 10, \n",
    "                    directory=samples_dir, prefix=\"epoch_%s\" % epoch)\n",
    "    \n",
    "    def _evaluate_per_batch(self, sess, data_X, data_Y, with_updates=False):\n",
    "        if with_updates:\n",
    "            summary, _, cost = sess.run([self.merged, self.updates, self.accuracy], feed_dict={self.X: data_X, self.Y: data_Y,\n",
    "                                                                         self.learning_rate: self.hparams['learning_rate']})            \n",
    "        else:\n",
    "            summary, cost = sess.run([self.merged, self.accuracy], feed_dict={self.X: data_X, self.Y: data_Y})        \n",
    "        return summary, cost\n",
    "    \n",
    "    def _evaluate_over_batches(self, sess, data, num_batches, epoch, writer, with_updates=False):\n",
    "        acc_epoch = np.zeros(num_batches)\n",
    "        for i in range(num_batches):\n",
    "            data_x, data_y = data.next_batch(self.hparams['batch_size'])\n",
    "            summary, cost = self._evaluate_per_batch(sess, self._binarise(data_x), data_y, with_updates)\n",
    "            acc_epoch[i] = cost[0]\n",
    "            writer.add_summary(summary, (epoch-1) * num_batches + i)\n",
    "#             if i % 5000 == 0:\n",
    "#                 print (np.mean(acc_epoch))\n",
    "        return acc_epoch\n",
    "        \n",
    "            \n",
    "    def train_and_val(self, sess, train_data, val_data, saver, model_dir, samples_dir):\n",
    "        num_batches = np.shape(train_data.images)[0] / self.hparams['batch_size']\n",
    "        width = height = np.sqrt(np.shape(train_data.images)[1])\n",
    "        \n",
    "        for epoch in range(1, self.hparams['num_epochs'] + 1):     \n",
    "            # Training part        \n",
    "            train_acc_epoch = self._evaluate_over_batches(sess, train_data, num_batches, epoch, self.train_writer, with_updates=True)\n",
    "                \n",
    "            self.stats['train_batch_acc'][epoch] = train_acc_epoch\n",
    "            self.stats['train_mean_acc'][epoch] = np.mean(train_acc_epoch)\n",
    "            \n",
    "            # Validation part\n",
    "            val_acc_epoch = self._evaluate_over_batches(sess, val_data, num_batches, epoch, self.train_writer, with_updates=False)\n",
    "            \n",
    "            self.stats['val_batch_acc'][epoch] = train_acc_epoch\n",
    "            self.stats['val_mean_acc'][epoch] = np.mean(val_acc_epoch)\n",
    "            \n",
    "            save_path = saver.save(sess, model_dir+self.model_name+\"_\"+str(date.today())+\"_\"+str(epoch)+\"_\"+str(0)+\"_model.ckpt\")\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "            \n",
    "            print(\"Epoch number %d\" % epoch)\n",
    "            print(\"Training average loss: %f\" % self.stats['train_mean_acc'][epoch])\n",
    "            print(\"Validate average loss: %f\" % self.stats['val_mean_acc'][epoch])\n",
    "            \n",
    "#             if self.data_type==\"images\":                \n",
    "#                 self.save_samples(samples_dir, epoch, height, width)            \n",
    "                \n",
    "                                    \n",
    "    def _construct_model(self):\n",
    "        model = self._construct_model_internal()\n",
    "        return model\n",
    "            \n",
    "    def _construct_model_internal(self):\n",
    "        model = {}\n",
    "        model['Y_all_classes'] = tf.diag(tf.ones(self.num_classes, 1))\n",
    "        model['U_all_y'] = tf.reshape(tf.matmul(self.U, model['Y_all_classes']), [1, self.num_hidden, self.num_classes])\n",
    "        model['WX'] , model['O_all'], model['positive_part'], model['p_y_all_given_x'] = self._give_p_all_y_given_x(model['U_all_y'], model['Y_all_classes'])\n",
    "\n",
    "        # Calculate p(y|x) for concrete x\n",
    "        # Result : p_y_given_x (batch_dimension x 1)\n",
    "        model['p_y_given_x'] = tf.reshape(tf.reduce_sum(tf.mul(model['p_y_all_given_x'], self.Y),1), [-1, 1])        \n",
    "\n",
    "        # Training part\n",
    "\n",
    "        # Calculate UY\n",
    "        # Result : U (batch_dimension x num_hidden)\n",
    "        model['UY']= tf.transpose(tf.matmul(self.U, tf.transpose(self.Y)))\n",
    "        \n",
    "        model['O'] = model['WX'] + model['UY']\n",
    "\n",
    "        # O_sigma: (batch_dimension x num_hidden)\n",
    "        model['O_sigma'] = tf.sigmoid(model['O'])\n",
    "\n",
    "        # O_sigma_all_Y : (batch_dimension x num_hidden x num_classes)\n",
    "        model['O_sigma_all_Y'] = tf.sigmoid(model['O_all'])\n",
    "\n",
    "        # O_sigma_all_Y_p : (batch_dimension x num_hidden x num_classes)        \n",
    "        model['O_sigma_all_Y_p'] = tf.mul(model['O_sigma_all_Y'], tf.tile(tf.reshape(model['p_y_all_given_x'], [-1, 1, self.num_classes]), [1, self.num_hidden, 1]))                         \n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def _define_gradients(self):\n",
    "        d_U = tf.zeros([self.num_hidden, self.num_classes], dtype=tf.float32)\n",
    "        d_W = tf.zeros([self.num_hidden, self.num_visible], dtype=tf.float32)\n",
    "        d_b = tf.zeros([self.num_visible, 1], dtype=tf.float32)\n",
    "        d_c = tf.zeros([self.num_hidden, 1], dtype=tf.float32)\n",
    "        d_d = tf.zeros([self.num_classes, 1], dtype=tf.float32)            \n",
    "        \n",
    "        if self.model_name == 'grbm' or self.model_name == 'hybrid':\n",
    "            # Generative gradients\n",
    "            d_U_gen, d_W_gen, d_b_gen, d_c_gen, d_d_gen = self._calc_generative_grads(self.Y, self.X)\n",
    "            d_U = d_U + self.gen * d_U_gen\n",
    "            d_W = d_W + self.gen * d_W_gen\n",
    "            d_b = d_b + self.gen * d_b_gen\n",
    "            d_c = d_c + self.gen * d_c_gen\n",
    "            d_d = d_d + self.gen * d_d_gen\n",
    "        \n",
    "        if self.model_name == 'drbm' or self.model_name == 'hybrid':\n",
    "            # Discriminative gradients        \n",
    "            # # d_U: (num_hidden x num_classes)\n",
    "            print(self.model['O_sigma'])\n",
    "            dU_left = tf.matmul(tf.transpose(self.model['O_sigma']), self.Y)\n",
    "            dU_right = tf.matmul(tf.transpose(tf.reduce_sum(self.model['O_sigma_all_Y_p'], 2)), self.Y)\n",
    "            d_U_disc = self.batch_normalization * (dU_left - dU_right)\n",
    "            d_U_disc = tf.reshape(d_U_disc, [self.num_hidden, self.num_classes])\n",
    "\n",
    "            # d_W : (num_hidden x num_visible)\n",
    "            dW_left = tf.matmul(tf.transpose(self.model['O_sigma']), self.X)\n",
    "            dW_right = tf.matmul(tf.transpose(tf.reduce_sum(self.model['O_sigma_all_Y_p'], 2)), self.X) \n",
    "        \n",
    "            d_W_disc = self.batch_normalization * (dW_left - dW_right)\n",
    "            d_W_disc = tf.reshape(d_W_disc, [self.num_hidden, self.num_visible])\n",
    "\n",
    "            # d_c : (num_hidden x 1)\n",
    "            dc_left = tf.reduce_sum(self.model['O_sigma'], 0)\n",
    "            dc_right = tf.reduce_sum(tf.reduce_sum(self.model['O_sigma_all_Y_p'], 2), 0)\n",
    "            d_c_disc = self.batch_normalization * (dc_left - dc_right)\n",
    "            d_c_disc = tf.reshape(d_c_disc, [self.num_hidden, 1])\n",
    "\n",
    "            # d_d : (num_classes x 1)\n",
    "            d_d_disc = self.batch_normalization * (tf.reduce_sum(self.Y - self.model['p_y_all_given_x'], 0))\n",
    "            d_d_disc = tf.reshape(d_d_disc, [self.num_classes, 1])\n",
    "            \n",
    "            d_U = d_U + self.discr * d_U_disc\n",
    "            d_W = d_W + self.discr * d_W_disc\n",
    "            d_c = d_c + self.discr * d_c_disc\n",
    "            d_d = d_d + self.discr * d_d_disc\n",
    "        \n",
    "        return d_U, d_W, d_b, d_c, d_d\n",
    "        \n",
    "        \n",
    "    def _give_p_all_y_given_x(self, U_all_y, Y_all_classes):\n",
    "        # Tensor of shape (None x num_hidden)\n",
    "        WX = tf.transpose(tf.matmul(self.W, tf.transpose(self.X)))\n",
    "    \n",
    "    \n",
    "        # WX + c for all batches\n",
    "        # Result : O (batch_dimension x num_hidden)\n",
    "        O = WX + tf.reshape(self.c, [1, self.num_hidden])\n",
    "    \n",
    "        # Resulted O:\n",
    "        # Result: O (batch_dimension x num_hidden x num_classes)\n",
    "        O = tf.reshape(O, [-1, self.num_hidden, 1]) + tf.reshape(U_all_y, [1, self.num_hidden, self.num_classes])\n",
    "\n",
    "        # First term in log p(y|x) which is calculated for each x in the batch\n",
    "        # Result : first_term (batch_dimension x num_classes)\n",
    "        first_term = tf.transpose(tf.matmul(Y_all_classes, self.d))\n",
    "    \n",
    "        # Second term in log p(y|x) which is calculated for each x in the batch\n",
    "        # Result : second_term (batch_dimension x num_classes)            \n",
    "        second_term = tf.reduce_sum(tf.nn.softplus(O), 1)\n",
    "    \n",
    "        # Positive part of log p(y|x)  \n",
    "        # Result: positive_part (batch_dimension x num_classes)\n",
    "        positive_part = first_term + second_term\n",
    "\n",
    "        # Use the softmax to calculate the probabilities:\n",
    "        # Result: p_y_all_given_x (batch_dimension x num_classes)\n",
    "        p_y_all_given_x = tf.nn.softmax(positive_part)\n",
    "    \n",
    "        return WX, O, positive_part, p_y_all_given_x            \n",
    "        \n",
    "        \n",
    "    def _create_placeholders(self):\n",
    "        X = tf.placeholder(tf.float32, [None, self.num_visible])\n",
    "        \n",
    "        Y = tf.placeholder(tf.float32, [None, self.num_classes])\n",
    "        \n",
    "        learning_rate = tf.placeholder(tf.float32)\n",
    "        \n",
    "        return X, Y, learning_rate\n",
    "    \n",
    "    def _initialize_variables(self, m_sqrt):\n",
    "        U = tf.get_variable('U', [self.num_hidden, self.num_classes], tf.float32, \n",
    "                           tf.random_uniform_initializer(minval=-m_sqrt, maxval=m_sqrt, seed=self.seed, dtype=tf.float32), None)\n",
    "        \n",
    "        W = tf.get_variable('W', [self.num_hidden, self.num_visible], tf.float32, \n",
    "                           tf.random_uniform_initializer(minval=-m_sqrt, maxval=m_sqrt, seed=self.seed, dtype=tf.float32), None)\n",
    "        \n",
    "        b = tf.get_variable('b', [self.num_visible, 1], tf.float32,\n",
    "                           tf.zeros_initializer, None)\n",
    "        \n",
    "        c = tf.get_variable('c', [self.num_hidden, 1], tf.float32,\n",
    "                           tf.zeros_initializer, None)\n",
    "        \n",
    "        d = tf.get_variable('d', [self.num_classes, 1], tf.float32,\n",
    "                           tf.zeros_initializer, None)  \n",
    "        \n",
    "        return U, W, b, c, d\n",
    "    \n",
    "    def _sample_prob(self, probs, size):    \n",
    "        rand = tf.random_uniform([1, size], minval=0.0, maxval=1.0, dtype=tf.float32)\n",
    "        \n",
    "        return tf.cast(rand < probs, tf.float32)\n",
    "    \n",
    "#     def _sample_vec(self, probs, size):\n",
    "        \n",
    "        \n",
    "#         return tf.nn.relu(tf.sign(probs - rand))\n",
    "\n",
    "    def _calc_generative_grads(self, y, x):\n",
    "        y0, x0, h0, y1, x1, h1 = self._gibbs_sampling_step(y, x)\n",
    "        \n",
    "        d_U_gen = self.batch_normalization * ( tf.transpose(h0) * y0 - tf.transpose(h1) * y1)\n",
    "        d_W_gen = self.batch_normalization * ( tf.transpose(h0) * x0 - tf.transpose(h1) * x1)\n",
    "        \n",
    "        d_b_gen = self.batch_normalization * (tf.reduce_sum(x0 - x1, 0))\n",
    "        d_b_gen = tf.reshape(d_b_gen, [self.num_visible, 1])\n",
    "        \n",
    "        d_c_gen = self.batch_normalization * (tf.reduce_sum(h0 - h1,0))\n",
    "        d_c_gen = tf.reshape(d_c_gen, [self.num_hidden, 1])\n",
    "        \n",
    "        d_d_gen = self.batch_normalization * (tf.reduce_sum(y0 - y1, 0))\n",
    "        d_d_gen = tf.reshape(d_d_gen, [self.num_classes, 1])\n",
    "        \n",
    "        return d_U_gen, d_W_gen, d_b_gen, d_c_gen, d_d_gen\n",
    "        \n",
    "    \n",
    "    def _gibbs_sampling_step(self, y, x):\n",
    "        # Positive phase\n",
    "        y0 = y\n",
    "        x0 = x\n",
    "        h0 = tf.transpose(tf.nn.sigmoid(self.c + tf.matmul(self.W,tf.transpose(x0)) + tf.matmul(self.U, tf.transpose(y0))))        \n",
    "    \n",
    "        # Negative phase\n",
    "        h0new = self._sample_h(h0)    \n",
    "        y1 = self._sample_y(h0new)\n",
    "        x1 = self._sample_x(h0new)\n",
    "        h1 = tf.transpose(tf.nn.sigmoid(self.c + tf.matmul(self.W, tf.transpose(x1)) + tf.matmul(self.U, tf.transpose(y1))))\n",
    "    \n",
    "        return y0, x0, h0, y1, x1, h1\n",
    "\n",
    "    def _sample_h(self, h_prob):            \n",
    "#         return self._binarise(h_prob)\n",
    "        return self._sample_prob(h_prob, self.num_hidden)\n",
    "    def _sample_y(self, h):\n",
    "        yprob = tf.transpose(tf.nn.softmax(self.d + tf.matmul(tf.transpose(self.U), tf.transpose(h))))\n",
    "        \n",
    "#         y_sampled = tf.cast(tf.multinomial(yprob, 1), tf.int32)\n",
    "\n",
    "#         u = tf.random_uniform([self.hparams['batch_size']])\n",
    "        \n",
    "# #         u = tf.random_uniform([1, self.num_classes], minval=0.0, maxval=1.0, dtype=tf.float32)\n",
    "#         less_eq = tf.cast((u <= tf.cumsum(yprob, 1)), tf.int32)\n",
    "#         sample = tf.cast(tf.equal(less_eq, tf.ones(self.num_classes, tf.int32)), tf.float32)\n",
    "        \n",
    "#         return sample\n",
    "        return tf.squeeze(tf.gather(self.model['Y_all_classes'], tf.cast(tf.multinomial(yprob, 1), tf.int32)), [1])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "#         print(tf.tile(tf.reshape(tf.range(self.num_classes),[1,self.num_classes]),[-1,1]) == tf.tile(y_sampled, [-1,self.num_classes]))\n",
    "        \n",
    "        \n",
    "#         all_batches_y = tf.tile(tf.reshape(self.model['Y_all_classes'],[1, self.num_classes, self.num_classes]),[-1, 1, 1])\n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(tf.tile(tf.reshape(tf.range(self.num_classes),[1,self.num_classes]),[-1,1]) == y_sampled)\n",
    "\n",
    "#         return self.model['Y_all_classes'][tf.cast(tf.multinomial(yprob, 1), tf.int32)]\n",
    "#         return sefl._binarise(yprob)\n",
    "#         return self._sample_prob(yprob, self.num_classes)\n",
    "    def _sample_x(self, h):\n",
    "        xprob = tf.transpose(tf.nn.sigmoid(self.b + tf.matmul(tf.transpose(self.W), tf.transpose(h))))\n",
    "        return self._sample_prob(xprob, self.num_visible)\n",
    "    \n",
    "    def _binarise(self, images):\n",
    "        return (images > 0).astype('float32')\n",
    "#         return (np.random.uniform(size=images.shape) < images).astype('float32')    \n",
    "    def variable_summaries(var):\n",
    "        \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean', mean)\n",
    "            with tf.name_scope('stddev'):\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "                tf.summary.scalar('stddev', stddev)\n",
    "                tf.summary.scalar('max', tf.reduce_max(var))\n",
    "                tf.summary.scalar('min', tf.reduce_min(var))\n",
    "                tf.summary.histogram('histogram', var)\n",
    "    \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model HyperParameters\n",
    "num_visible = 28*28\n",
    "num_classes = 10\n",
    "num_hidden = 500\n",
    "batch_size = 1\n",
    "num_epochs = 100\n",
    "num_batches = np.shape(trX)[0] / batch_size\n",
    "\n",
    "if np.shape(trX)[0] % batch_size > 0:\n",
    "    num_batches += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(allow_soft_placement = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hparams = {}\n",
    "hparams['num_hidden'] = num_hidden\n",
    "hparams['batch_size'] = batch_size\n",
    "hparams['num_epochs'] = num_epochs\n",
    "hparams['learning_rate'] = 0.05\n",
    "hparams['seed'] = 123\n",
    "hparams['gen'] = 0.01\n",
    "hparams['discr'] = 1.0\n",
    "model_names= {'drbm', 'grbm', 'hybrid'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"drbm/Sigmoid:0\", shape=(?, 500), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config = config) as sess:\n",
    "    model = Model(sess, num_visible, num_classes, hparams, model_name='drbm', data_type='images', logs_dir='./logs')\n",
    "    model._build_model()\n",
    "#     sess.run(tf.initialize_all_variables())\n",
    "    model.train_and_val(sess, mnist.train, mnist.validation, tf.train.Saver(), './models/', './samples/')\n",
    "    model.train(sess, mnist.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing of all the models :P\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
