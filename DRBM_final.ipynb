{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminative Restricted Boltzmann Machines TensorFlow Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.misc\n",
    "from logging import getLogger\n",
    "import datetime\n",
    "import dateutil.tz\n",
    "from datetime import date\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "import pprint\n",
    "import tarfile\n",
    "\n",
    "import scipy.misc\n",
    "\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, sess, conf, checkpoint_fname=None):\n",
    "        self.sess = sess\n",
    "        \n",
    "        # Model input parameters\n",
    "        self.num_hidden = conf.num_hidden\n",
    "        self.num_visible = conf.num_visible\n",
    "        self.num_classes = conf.num_classes\n",
    "        \n",
    "        # Learning hyperparameters\n",
    "        self.hparams = {}\n",
    "        self.hparams['batch_size'] = conf.batch_size\n",
    "        self.hparams['num_epochs'] = conf.num_epochs\n",
    "        self.hparams['learning_rate'] = conf.learning_rate\n",
    "        ## Generative objective weight\n",
    "        self.hparams['alpha'] = conf.alpha\n",
    "        \n",
    "        # Internal stuff\n",
    "        self.seed = conf.seed\n",
    "        \n",
    "        # Logging and saving parameters\n",
    "        self.model_name = conf.model_name\n",
    "        self.logs_dir = conf.logs_dir\n",
    "        self.model_type = conf.model_type\n",
    "        self.model_dir = conf.model_dir\n",
    "        \n",
    "        self._build_model()\n",
    "        \n",
    "        \n",
    "        with tf.variable_scope('summary'):\n",
    "            scalar_summary_tags = ['training_accuracy', 'validation_accuracy']\n",
    "            \n",
    "            self.summary_placeholders = {}\n",
    "            self.summary_ops = {}\n",
    "            \n",
    "            for tag in scalar_summary_tags:\n",
    "                self.summary_placeholders[tag] = tf.placeholder('float32', None, name=tag)\n",
    "                self.summary_ops[tag]  = tf.scalar_summary(tag, self.summary_placeholders[tag])\n",
    "                \n",
    "            self.writer = tf.train.SummaryWriter(os.path.join(self.model_dir, 'logs') , self.sess.graph)\n",
    "                \n",
    "        self.merged = tf.merge_all_summaries()\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        tf.initialize_all_variables().run()\n",
    "        \n",
    "        self._load_model_from_checkpoint(checkpoint_fname)            \n",
    "        \n",
    "    def _load_model_from_checkpoint(self, checkpoint_fname=None):\n",
    "        print(\" [*] Loading checkpoints...\")\n",
    "        \n",
    "        if checkpoint_fname is not None:\n",
    "            self.saver.restore(self.sess, checkpoint_fname)\n",
    "            return True        \n",
    "        else:\n",
    "            ckpt = tf.train.get_checkpoint_state(self.model_dir)    \n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "                fname = os.path.join(self.model_dir, ckpt_name)\n",
    "                self.saver.restore(self.sess, fname)\n",
    "                print(\" [*] Load SUCCESS: %s\" % fname)\n",
    "                return True\n",
    "            else:\n",
    "                print(\" [!] Load FAILED: %s\" % self.model_dir)\n",
    "            return False\n",
    "        \n",
    "    def save_model_to_checkpoint(self, step):\n",
    "        print(\" [*] Saving checkpoints...\")\n",
    "        model_name = type(self).__name__\n",
    "\n",
    "        if not os.path.exists(self.model_dir):\n",
    "            os.makedirs(self.model_dir)\n",
    "            \n",
    "        return self.saver.save(self.sess, self.model_dir, global_step=step)\n",
    "    \n",
    "    \n",
    "    def _build_model(self, with_init=True):\n",
    "        self.model = {}\n",
    "        with tf.variable_scope(self.model_name):\n",
    "            self.model['X'], self.model['Y'], self.model['learning_rate'] = self._create_placeholders()\n",
    "            \n",
    "            m = np.max([self.num_hidden, self.num_classes, self.num_visible])\n",
    "            \n",
    "            self.model['U'], self.model['W'], self.model['b'], self.model['c'], self.model['d'] = self._create_matrices(1./np.sqrt(m))\n",
    "            \n",
    "            # Defines the internal \n",
    "            self._construct_internal_variables()\n",
    "            \n",
    "            self.model['d_U'], self.model['d_W'], self.model['d_b'], self.model['d_c'], self.model['d_d'] = self._define_gradients()\n",
    "            \n",
    "            self.updates = [self.model['U'].assign_add(self.model['learning_rate'] * self.model['d_U']),\n",
    "                                self.model['W'].assign_add(self.model['learning_rate'] * self.model['d_W']),\n",
    "                                self.model['b'].assign_add(self.model['learning_rate'] * self.model['d_b']),\n",
    "                                self.model['c'].assign_add(self.model['learning_rate'] * self.model['d_c']),\n",
    "                                self.model['d'].assign_add(self.model['learning_rate'] * self.model['d_d'])]\n",
    "            \n",
    "            self.predicted_y = tf.argmax(self.model['p_y_all_given_x'], 1)\n",
    "            \n",
    "            self.ground_truth = tf.argmax(self.model['Y'], 1)\n",
    "            \n",
    "            self.correct_prediction = tf.equal(self.predicted_y, self.ground_truth, name='correct_prediction')\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32), keep_dims=True, name='accuracy')\n",
    "\n",
    "    \n",
    "    def _construct_internal_variables(self):\n",
    "        # Tensor of shape (num_classes x num_classes)\n",
    "        self.model['Y_all_classes'] = tf.diag(tf.ones(self.num_classes, 1), name='all_the_classes')\n",
    "        \n",
    "        # tensor of shape (num_hidden x num_classes)\n",
    "        self.model['U_all_y'] = tf.matmul(self.model['U'], self.model['Y_all_classes'])\n",
    "        \n",
    "        self.model['WX'] , self.model['O_all'], self.model['positive_part'], self.model['p_y_all_given_x'] = self._give_p_all_y_given_x()\n",
    "        \n",
    "        if self.model_type == 'drbm' or self.model_type == 'hybrid':            \n",
    "            # Calculate p(y|x) for concrete x\n",
    "            # Result : p_y_given_x (None x 1)\n",
    "            self.model['p_y_given_x'] = tf.reshape(tf.reduce_sum(tf.mul(self.model['p_y_all_given_x'], self.model['Y']), 1), [-1, 1])\n",
    "            \n",
    "            # Training part\n",
    "\n",
    "            # Calculate UY\n",
    "            # Result : U (None x num_hidden)\n",
    "            self.model['UY'] = tf.matmul(self.model['Y'], tf.transpose(self.model['U']), name='UY')\n",
    "            \n",
    "            self.model['O'] = self.model['WX'] + self.model['UY']\n",
    "\n",
    "            # O_sigma: (None x num_hidden)\n",
    "            self.model['O_sigma'] = tf.sigmoid(self.model['O'])\n",
    "\n",
    "            # O_sigma_all_Y : (None x num_hidden x num_classes)\n",
    "            self.model['O_sigma_all_Y'] = tf.sigmoid(self.model['O_all'])\n",
    "\n",
    "            # O_sigma_all_Y_p : (None x num_hidden x num_classes)        \n",
    "            self.model['O_sigma_all_Y_p'] = tf.mul(self.model['O_sigma_all_Y'], tf.tile(tf.reshape(self.model['p_y_all_given_x'], [-1, 1, self.num_classes]), [1, self.num_hidden, 1]))                         \n",
    "            \n",
    "    \n",
    "    def _give_p_all_y_given_x(self):\n",
    "        # Tensor of shape (None x num_hidden)\n",
    "        WX = tf.matmul(self.model['X'], tf.transpose(self.model['W']), name='WX')    \n",
    "    \n",
    "        # WX + c for all batches\n",
    "        # Result : O (None x num_hidden)\n",
    "        O = WX + tf.transpose(self.model['c'])\n",
    "        \n",
    "        # Resulted O:\n",
    "        # Result: O (None x num_hidden x num_classes)   \n",
    "        O = tf.reshape(O, [-1, self.num_hidden, 1]) + tf.reshape(self.model['U_all_y'], [1, self.num_hidden, self.num_classes])\n",
    "\n",
    "        # First term in log p(y|x) which is calculated for each x in the batch\n",
    "        # Result : first_term (1 x num_classes)\n",
    "        first_term = tf.matmul(tf.transpose(self.model['d']), self.model['Y_all_classes'])       \n",
    "        \n",
    "        # Second term in log p(y|x) which is calculated for each x in the batch\n",
    "        # Result : second_term (None x num_classes)            \n",
    "        second_term = tf.reduce_sum(tf.nn.softplus(O), 1)        \n",
    "    \n",
    "        # Positive part of log p(y|x)  \n",
    "        # Result: positive_part (None x num_classes)\n",
    "        positive_part = first_term + second_term\n",
    "        \n",
    "        # Use the softmax to calculate the probabilities:\n",
    "        # Result: p_y_all_given_x (None x num_classes)\n",
    "        p_y_all_given_x = tf.nn.softmax(positive_part)\n",
    "    \n",
    "        return WX, O, positive_part, p_y_all_given_x   \n",
    "        \n",
    "    def _define_gradients(self):\n",
    "        d_U = tf.zeros([self.num_hidden, self.num_classes], dtype=tf.float32, name='d_U')\n",
    "        d_W = tf.zeros([self.num_hidden, self.num_visible], dtype=tf.float32, name='d_W')\n",
    "        d_b = tf.zeros([self.num_visible, 1], dtype=tf.float32, name='d_b')\n",
    "        d_c = tf.zeros([self.num_hidden, 1], dtype=tf.float32, name='d_c')\n",
    "        d_d = tf.zeros([self.num_classes, 1], dtype=tf.float32, name='d_d')         \n",
    "        \n",
    "        if self.model_type == 'grbm' or self.model_type == 'hybrid':\n",
    "            # Generative gradients\n",
    "            d_U_gen, d_W_gen, d_b_gen, d_c_gen, d_d_gen = self._calc_generative_grads(self.model['Y'], self.model['X'])\n",
    "            if self.model_type == 'grbm':                    \n",
    "                d_U = d_U_gen\n",
    "                d_W = d_W_gen\n",
    "                d_b = d_b_gen\n",
    "                d_c = d_c_gen\n",
    "                d_d = d_d_gen\n",
    "            elif self.model_type == 'hybrid':\n",
    "                d_U = d_U + self.hparams['alpha']* d_U_gen\n",
    "                d_W = d_W + self.hparams['alpha'] * d_W_gen\n",
    "                d_b = d_b + self.hparams['alpha'] * d_b_gen\n",
    "                d_c = d_c + self.hparams['alpha'] * d_c_gen\n",
    "                d_d = d_d + self.hparams['alpha'] * d_d_gen\n",
    "                \n",
    "        \n",
    "        if self.model_type == 'drbm' or self.model_type == 'hybrid':\n",
    "            # Discriminative gradients        \n",
    "            # # d_U: (num_hidden x num_classes)\n",
    "            dU_left = tf.matmul(tf.transpose(self.model['O_sigma']), self.model['Y'])\n",
    "            dU_right = tf.matmul(tf.transpose(tf.reduce_sum(self.model['O_sigma_all_Y_p'], 2)), self.model['Y'])\n",
    "            d_U_disc = tf.div(dU_left - dU_right, self.hparams['batch_size'])\n",
    "            d_U_disc = tf.reshape(d_U_disc, [self.num_hidden, self.num_classes])\n",
    "            \n",
    "            # d_W : (num_hidden x num_visible)\n",
    "            dW_left = tf.matmul(tf.transpose(self.model['O_sigma']), self.model['X'])\n",
    "            dW_right = tf.matmul(tf.transpose(tf.reduce_sum(self.model['O_sigma_all_Y_p'], 2)), self.model['X']) \n",
    "        \n",
    "            d_W_disc = tf.div(dW_left - dW_right, self.hparams['batch_size'])\n",
    "            d_W_disc = tf.reshape(d_W_disc, [self.num_hidden, self.num_visible])\n",
    "\n",
    "            # d_c : (num_hidden x 1)\n",
    "            dc_left = tf.reduce_sum(self.model['O_sigma'], 0)\n",
    "            dc_right = tf.reduce_sum(tf.reduce_sum(self.model['O_sigma_all_Y_p'], 2), 0)\n",
    "            d_c_disc = tf.div(dc_left - dc_right, self.hparams['batch_size'])\n",
    "            d_c_disc = tf.reshape(d_c_disc, [self.num_hidden, 1])\n",
    "\n",
    "            # d_d : (num_classes x 1)\n",
    "            d_d_disc = tf.div(tf.reduce_sum(self.model['Y'] - self.model['p_y_all_given_x'], 0), self.hparams['batch_size'])\n",
    "            d_d_disc = tf.reshape(d_d_disc, [self.num_classes, 1])\n",
    "            \n",
    "            d_U = d_U + d_U_disc\n",
    "            d_W = d_W + d_W_disc\n",
    "            d_c = d_c + d_c_disc\n",
    "            d_d = d_d + d_d_disc\n",
    "        \n",
    "        return d_U, d_W, d_b, d_c, d_d\n",
    "        \n",
    "                 \n",
    "\n",
    "    def _calc_generative_grads(self, y, x):\n",
    "        y0, x0, h0, y1, x1, h1 = self._gibbs_sampling_step(y, x)\n",
    "                \n",
    "        h0 = tf.reshape(h0, [-1, self.num_hidden, 1])\n",
    "        y0 = tf.reshape(y0, [-1, self.num_classes, 1])\n",
    "        x0 = tf.reshape(x0, [-1, self.num_visible, 1])\n",
    "        h1 = tf.reshape(h1, [-1, self.num_hidden, 1])\n",
    "        y1 = tf.reshape(y1, [-1, self.num_classes, 1])\n",
    "        x1 = tf.reshape(x1, [-1, self.num_visible, 1])\n",
    "        \n",
    "        d_U_gen = tf.reduce_mean(tf.batch_matmul(h0, y0, adj_y = True) - tf.batch_matmul(h1, y1, adj_y = True), 0)\n",
    "        d_W_gen = tf.reduce_mean(tf.batch_matmul(h0, x0, adj_y = True) - tf.batch_matmul(h1, x1, adj_y = True), 0)\n",
    "        \n",
    "        d_b_gen = tf.reduce_sum(x0 - x1, 0)\n",
    "        d_c_gen = tf.reduce_sum(h0 - h1, 0)\n",
    "        d_d_gen = tf.reduce_sum(y0 - y1, 0)        \n",
    "        \n",
    "        return d_U_gen, d_W_gen, d_b_gen, d_c_gen, d_d_gen\n",
    "        \n",
    "    \n",
    "    def _gibbs_sampling_step(self, y, x):\n",
    "        # Positive phase\n",
    "        y0 = y\n",
    "        x0 = x\n",
    "        h0 = tf.nn.sigmoid(tf.transpose(self.model['c'] + tf.matmul(self.model['W'], tf.transpose(x0)) + tf.matmul(self.model['U'], tf.transpose(y0))))\n",
    "    \n",
    "        # Negative phase\n",
    "        h0new = self._sample_h(h0)    \n",
    "        y1 = self._sample_y(h0new)\n",
    "        x1 = self._sample_x(h0new)\n",
    "        h1 = tf.nn.sigmoid(tf.transpose(self.model['c'] + tf.matmul(self.model['W'], tf.transpose(x1)) + tf.matmul(self.model['U'], tf.transpose(y1))))\n",
    "    \n",
    "        return y0, x0, h0, y1, x1, h1\n",
    "    \n",
    "    def _sample_prob(self, probs, size):\n",
    "        rand = tf.random_uniform([self.hparams['batch_size'], size], minval=0.0, maxval=1.0, dtype=tf.float32)        \n",
    "        return tf.cast(rand < probs, tf.float32)\n",
    "\n",
    "    def _sample_h(self, h_prob):            \n",
    "        return self._sample_prob(h_prob, self.num_hidden)\n",
    "    \n",
    "    def _sample_y(self, h):\n",
    "        yprob = tf.nn.softmax(tf.transpose(self.model['d'] + tf.matmul(tf.transpose(self.model['U']), tf.transpose(h))), dim=-1)\n",
    "        squeezed_y = tf.squeeze(tf.one_hot(tf.multinomial(yprob,1), self.num_classes), [1])    \n",
    "        return tf.matmul(squeezed_y, self.model['Y_all_classes'])    \n",
    "\n",
    "    def _sample_x(self, h):\n",
    "        xprob = tf.nn.sigmoid(tf.transpose(self.model['b'] + tf.matmul(tf.transpose(self.model['W']), tf.transpose(h))))\n",
    "        return self._sample_prob(xprob, self.num_visible)\n",
    "    \n",
    "    def _create_placeholders(self):\n",
    "        X = tf.placeholder(tf.float32, [None, self.num_visible])\n",
    "        \n",
    "        Y = tf.placeholder(tf.float32, [None, self.num_classes])\n",
    "        \n",
    "        learning_rate = tf.placeholder(tf.float32)\n",
    "        \n",
    "        return X, Y, learning_rate\n",
    "    \n",
    "    def _create_matrices(self, m_sqrt):\n",
    "        U = tf.get_variable('U', [self.num_hidden, self.num_classes], tf.float32, \n",
    "                           tf.random_uniform_initializer(minval=-m_sqrt, maxval=m_sqrt, seed=self.seed, dtype=tf.float32), None)\n",
    "        \n",
    "        W = tf.get_variable('W', [self.num_hidden, self.num_visible], tf.float32, \n",
    "                           tf.random_uniform_initializer(minval=-m_sqrt, maxval=m_sqrt, seed=self.seed, dtype=tf.float32), None)\n",
    "        \n",
    "        b = tf.get_variable('b', [self.num_visible, 1], tf.float32,\n",
    "                           tf.zeros_initializer, None)\n",
    "        \n",
    "        c = tf.get_variable('c', [self.num_hidden, 1], tf.float32,\n",
    "                           tf.zeros_initializer, None)\n",
    "        \n",
    "        d = tf.get_variable('d', [self.num_classes, 1], tf.float32,\n",
    "                           tf.zeros_initializer, None)  \n",
    "        \n",
    "        return U, W, b, c, d\n",
    "    \n",
    "    def inject_summary(self, tag_dict, step):\n",
    "        summary_str_lists = self.sess.run([self.summary_ops[tag] for tag in tag_dict.keys()], {\n",
    "                self.summary_placeholders[tag]: value for tag, value in tag_dict.items()\n",
    "            })\n",
    "        \n",
    "        for summary_str in summary_str_lists:\n",
    "            self.writer.add_summary(summary_str, step)               \n",
    "                \n",
    "            \n",
    "    def _get_timestamp(self):\n",
    "        now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
    "        return now.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "    \n",
    "\n",
    "    def train(self, sess, data, with_update=False, debug_learning_rate=None):\n",
    "        num_batches = np.shape(data.images)[0] / self.hparams['batch_size']\n",
    "        \n",
    "        \n",
    "        accuracies = np.zeros(num_batches)\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            x, y = data.next_batch(self.hparams['batch_size'])\n",
    "            \n",
    "            if with_update==True:\n",
    "                if debug_learning_rate is not None:                    \n",
    "                    _, accuracy = self.sess.run([self.updates, self.accuracy], feed_dict={self.model['X'] : self._binarise(x),\n",
    "                                                                                          self.model['Y'] : y,\n",
    "                                                                                          self.model['learning_rate'] : debug_learning_rate})\n",
    "                else:\n",
    "                    _, accuracy = self.sess.run([self.updates, self.accuracy], feed_dict={self.model['X'] : self._binarise(x),\n",
    "                                                                                          self.model['Y'] : y,\n",
    "                                                                                          self.model['learning_rate'] : model.hparams['learning_rate']})\n",
    "                    \n",
    "            else:\n",
    "                accuracy = self.sess.run([self.accuracy], feed_dict={self.model['X'] : self._binarise(x),\n",
    "                                                                     self.model['Y'] : y})\n",
    "                \n",
    "            accuracies[i] = accuracy[0]\n",
    "        \n",
    "        return np.mean(accuracies)\n",
    "    \n",
    "    def test(self, sess, data):\n",
    "        accuracy =  self.sess.run([self.accuracy], feed_dict={self.model['X'] : self._binarise(data.images), self.model['Y'] : data.labels})\n",
    "        return accuracy[0]\n",
    "                                                 \n",
    "    \n",
    "    \n",
    "    def _binarise(self, images):\n",
    "        return (images > 0).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "\n",
    "# Model input parameters\n",
    "flags.DEFINE_integer(\"num_hidden\", 6002, \"number of hidden units\")\n",
    "flags.DEFINE_integer(\"num_visible\", 28 * 28, \"number of visible units\")\n",
    "flags.DEFINE_integer(\"num_classes\", 10, \"number of classes\")\n",
    "        \n",
    "# Learning hyperparameters\n",
    "flags.DEFINE_integer(\"batch_size\", 1, \"batch size\")\n",
    "flags.DEFINE_integer(\"num_epochs\", 70, \"number of epochs\")\n",
    "flags.DEFINE_float(\"learning_rate\", 0.005, \"learning rate\")\n",
    "flags.DEFINE_float(\"alpha\", 0.01, \"generative objective weight\")\n",
    "\n",
    "# Debug\n",
    "flags.DEFINE_string(\"model_name\", \"my_model\", \"name of the model\")\n",
    "flags.DEFINE_string(\"model_type\", \"grbm\", \"type of the model : [drbm, grbm, hybrid]\")\n",
    "flags.DEFINE_string(\"model_dir\", \"./debug/models/\", \"directory of saved checkpoints\")\n",
    "flags.DEFINE_string(\"logs_dir\", \"./debug/logs/\", \"directory to save the logs\")\n",
    "flags.DEFINE_integer(\"seed\", 123, \"random seed for python\")\n",
    "\n",
    "conf = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST', one_hot=True)\n",
    "\n",
    "next_train_batch = lambda x: mnist.train.next_batch(x)[0]\n",
    "next_validation_batch = lambda x: mnist.validation.next_batch(x)[0]\n",
    "next_test_batch = lambda x: mnist.test.next_batch(x)[0]\n",
    "\n",
    "def calc_gpu_fraction(fraction_string):\n",
    "    idx, num = fraction_string.split('/')\n",
    "    idx, num = float(idx), float(num)\n",
    "    fraction = 1 / (num - idx + 1)\n",
    "    print \" [*] GPU : %.4f\" % fraction\n",
    "    \n",
    "    return fraction\n",
    "gpu_options = tf.GPUOptions(\n",
    "      per_process_gpu_memory_fraction=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [*] Loading checkpoints...\n",
      " [!] Load FAILED: ./debug/models/grbm_lr_0.005_nh_6002/\n",
      "Epoch  0  is starting...\n",
      " [*] Loading checkpoints...\n",
      " [!] Load FAILED: ./debug/models/grbm_lr_0.005_nh_6002/\n",
      "Training...\n"
     ]
    }
   ],
   "source": [
    "debug_results = 'debug_results.csv'\n",
    "\n",
    "# learning_rates = [0.05, 0.01, 0.005, 0.001, 0.0005]\n",
    "# num_hiddens = [100, 200, 600, 1000, 6000]\n",
    "learning_rates = [0.005]\n",
    "num_hiddens = [6002]\n",
    "i  = 0\n",
    "\n",
    "for lr in learning_rates:\n",
    "    j = 0\n",
    "    for nh in num_hiddens:\n",
    "        conf.learning_rate = lr\n",
    "        conf.num_hidden = nh\n",
    "        conf.model_name = str(conf.model_type) + '_lr_' + str(lr) + '_nh_' + str(nh)\n",
    "        my_scope = str(conf.model_type) + str(i) + '_' + str(j)\n",
    "        conf.model_dir = './debug/models/'+conf.model_name + '/'\n",
    "\n",
    "        with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)) as sess:\n",
    "            with tf.variable_scope(my_scope) as scope:\n",
    "                with tf.device('/device:GPU:0'):\n",
    "                    model = Model(sess, conf)\n",
    "            \n",
    "                    epoch_train_accuracy = np.zeros(conf.num_epochs)\n",
    "                    epoch_validation_accuracy = np.zeros(conf.num_epochs)\n",
    "                    epoch_test_accuracy = np.zeros(conf.num_epochs)\n",
    "                    \n",
    "                    # start_epoch = 16\n",
    "            \n",
    "                    for epoch in range(conf.num_epochs):\n",
    "                        print 'Epoch ', epoch, ' is starting...'\n",
    "                        model._load_model_from_checkpoint()\n",
    "                \n",
    "                        print 'Training...'\n",
    "                        avg_train_acc = model.train(sess, mnist.train, with_update=True)\n",
    "                        print 'Average training accuracy : ', avg_train_acc\n",
    "                        epoch_train_accuracy[epoch] = avg_train_acc\n",
    "                \n",
    "                        model.inject_summary({'training_accuracy' : avg_train_acc}, epoch)\n",
    "                \n",
    "                        print 'Validation...'\n",
    "                        avg_val_acc = model.train(sess, mnist.validation, with_update=False)\n",
    "                        print 'Average validation accuracy : ', avg_val_acc\n",
    "                        epoch_validation_accuracy[epoch] = avg_val_acc\n",
    "                \n",
    "                        model.inject_summary({'validation_accuracy' : avg_val_acc}, epoch)\n",
    "                \n",
    "                        stop_training = False\n",
    "                \n",
    "                        if epoch > 5:\n",
    "                            stop_training = True\n",
    "                            for k in range(5):\n",
    "                                if epoch_validation_accuracy[epoch - k]  <= epoch_validation_accuracy[epoch - (k+1)]:\n",
    "                                    stop_training = False\n",
    "                \n",
    "                        print 'Testing...'\n",
    "                        test_acc = model.test(sess, mnist.test)\n",
    "                        print 'Testing accuracy : ', test_acc\n",
    "                        epoch_test_accuracy[epoch] = test_acc\n",
    "                \n",
    "                        print \"Saving checkpoints...\"\n",
    "                        save_path = model.save_model_to_checkpoint(epoch)                \n",
    "                        print \"Checkpoint succesfully saved...\"\n",
    "                \n",
    "                        with open(debug_results, 'a') as results_file:\n",
    "                            writer = csv.writer(results_file, delimiter=',')\n",
    "                            writer.writerow([conf.model_name, conf.model_type, epoch, conf.num_hidden,\n",
    "                                         conf.learning_rate,\n",
    "                                         avg_train_acc,\n",
    "                                         avg_val_acc,\n",
    "                                         test_acc, stop_training, save_path])                \n",
    "                                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
